{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Lexical Lullabies: LLM Based MIDI Generation with Music Theory Constraints\n",
        "Final Project for CAP6640 by Christian King"
      ],
      "metadata": {
        "id": "A9GeKAUopuWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 0: Libraries, Downloads, Prerequites\n",
        "---\n"
      ],
      "metadata": {
        "id": "6Xupa-HHqXvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we're just installing, importing, and setting up all the libraries and things we'll be using for the model.\n"
      ],
      "metadata": {
        "id": "Qj3BHPx4-CX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ds3W6PZGmi0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch miditok transformers accelerate datasets evaluate miditoolkit midi2audio rouge_score"
      ],
      "metadata": {
        "id": "REjl3VIXEZFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
        "from transformers import default_data_collator\n",
        "from datasets import load_dataset, Dataset as HFDataset\n",
        "from evaluate import load\n",
        "from tqdm.auto import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import miditok\n",
        "from miditok import REMI\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "_kgXSrty0fLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 1: Preprocessing\n",
        "---"
      ],
      "metadata": {
        "id": "XBPHLhnq0euP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Move Over MIDICaps Dataset:"
      ],
      "metadata": {
        "id": "bvB3y8Jp1SDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference in training time of the dataset being on google drive vs being local is huge. The difference in upload time from drive to local vs my computer to local is also huge. So through a lot of trial and error I found the best way of handling this is to upload a compressed version of the dataset to google drive and then move that to local and extract it here. \\\\\n",
        "\n",
        "So in my case I've stored the dataset at /content/drive/MyDrive/MidiCaps/midicaps.tar.gz\n",
        "along with the train.json and I'm just copying these over and then extracting them. I believe this is the fastest way to handle this in colab."
      ],
      "metadata": {
        "id": "iHTx0C0u8zV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rsync -av --progress /content/drive/MyDrive/MidiCaps/midicaps.tar.gz /content/\n",
        "!rsync -av --progress /content/drive/MyDrive/MidiCaps/train.json /content/\n",
        "!tar -xzvf /content/midicaps.tar.gz -C /content/"
      ],
      "metadata": {
        "id": "GdbR1xWw9nqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Check Dataset and Create Vocab\n",
        "Some midi files in this dataset aren't valid as they are missing the bare minimum requirements like a single pitch or duration token. So here we build functions to sort those files out and also build out this REMI token vocab that we are going to use to extend the Llama vocab."
      ],
      "metadata": {
        "id": "VIQ34s2P1kdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for essential midi token types\n",
        "def is_valid_midi_tokens(tokens):\n",
        "    has_pitch = any(token.startswith(\"Pitch_\") for token in tokens)\n",
        "    has_duration = any(token.startswith(\"Duration_\") for token in tokens)\n",
        "    has_velocity = any(token.startswith(\"Velocity_\") for token in tokens)\n",
        "\n",
        "    if not has_pitch:\n",
        "        return False, \"missing pitch token\"\n",
        "    if not has_duration:\n",
        "        return False, \"missing duration token\"\n",
        "    if not has_velocity:\n",
        "        return False, \"missing velocity token\"\n",
        "\n",
        "    return True, \"valid\"\n",
        "\n",
        "\n",
        "def clean_dataset_and_build_vocab(hf_dataset, base_path, midi_tokenizer, midi_max_length=256, sample_limit=20000):\n",
        "    # build valid sample list and vocabulary set\n",
        "    valid_samples = []\n",
        "    vocab_set = set()\n",
        "    n = min(len(hf_dataset), sample_limit)\n",
        "\n",
        "    for i in range(n):\n",
        "        sample = hf_dataset[i]\n",
        "        midi_path = os.path.join(base_path, sample[\"location\"])\n",
        "        try:\n",
        "            # encode midi file into token sequence\n",
        "            tok_seq = midi_tokenizer.encode(midi_path)\n",
        "            tokens = tok_seq[0].tokens[:midi_max_length]\n",
        "            valid, reason = is_valid_midi_tokens(tokens)\n",
        "            if not tokens or not valid:\n",
        "                print(f\"skipped sample {i}: {reason}\")\n",
        "                continue\n",
        "            midi_tokens = tokens.copy()\n",
        "\n",
        "            # ensure start/end markers are present\n",
        "            if \"<MIDI_START>\" not in midi_tokens:\n",
        "                midi_tokens.insert(0, \"<MIDI_START>\")\n",
        "            if \"<MIDI_END>\" not in midi_tokens:\n",
        "                midi_tokens.append(\"<MIDI_END>\")\n",
        "            sample[\"midi_tokens\"] = midi_tokens\n",
        "            valid_samples.append(sample)\n",
        "            vocab_set.update(midi_tokens)\n",
        "\n",
        "        except Exception as e:\n",
        "            # skip files that cause errors\n",
        "            print(f\"skipped sample {i} due to error: {e}\")\n",
        "\n",
        "    print(f\"total valid samples: {len(valid_samples)} / {n}\")\n",
        "\n",
        "    return valid_samples, sorted(vocab_set)\n",
        "\n",
        "\n",
        "def save_vocab(vocab_list, filepath):\n",
        "    # write vocabulary to file, one token per line\n",
        "    with open(filepath, \"w\") as f:\n",
        "        for token in vocab_list:\n",
        "            f.write(token + \"\\n\")\n",
        "\n",
        "    print(f\"vocab ({len(vocab_list)} tokens) saved to {filepath}\")\n",
        "\n",
        "\n",
        "def save_valid_samples(valid_samples, filepath):\n",
        "    # save valid samples as json\n",
        "    with open(filepath, \"w\") as f:\n",
        "        json.dump(valid_samples, f)\n",
        "\n",
        "    print(f\"saved {len(valid_samples)} samples to {filepath}\")\n",
        "\n",
        "\n",
        "def load_valid_samples(filepath):\n",
        "    # load samples from json file\n",
        "    with open(filepath, \"r\") as f:\n",
        "        valid_samples = json.load(f)\n",
        "\n",
        "    print(f\"loaded {len(valid_samples)} samples from {filepath}\")\n",
        "    return valid_samples\n"
      ],
      "metadata": {
        "id": "cFVYtppEQbxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Dataset\n",
        "Now that we have a our valid MIDI functions, we can actually create the dataset."
      ],
      "metadata": {
        "id": "LbOZcZBLST6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MidicapsMIDIDataset(Dataset):\n",
        "    def __init__(self, samples, base_path, tokenizer, midi_tokenizer, max_length=512, midi_max_length=256):\n",
        "        self.samples = samples  # list of data dicts\n",
        "        self.base_path = base_path  # root path for midi files\n",
        "        self.tokenizer = tokenizer  # text tokenizer\n",
        "        self.midi_tokenizer = midi_tokenizer  # midi tokenizer\n",
        "        self.max_length = max_length  # max tokens for text and midi\n",
        "        self.midi_max_length = midi_max_length  # max midi tokens\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        #remove spaces\n",
        "        caption = sample[\"caption\"].strip()\n",
        "        midi_tokens = sample.get(\"midi_tokens\", [])\n",
        "\n",
        "        if not midi_tokens:\n",
        "            # encode midi on the fly if not pre-tokenized\n",
        "            midi_path = os.path.join(self.base_path, sample[\"location\"])\n",
        "            try:\n",
        "                tok_seq = self.midi_tokenizer.encode(midi_path)\n",
        "                midi_tokens = tok_seq[0].tokens[:self.midi_max_length]\n",
        "\n",
        "                # add start/end markers if missing\n",
        "                if \"<MIDI_START>\" not in midi_tokens:\n",
        "                    midi_tokens.insert(0, \"<MIDI_START>\")\n",
        "                if \"<MIDI_END>\" not in midi_tokens:\n",
        "                    midi_tokens.append(\"<MIDI_END>\")\n",
        "\n",
        "            except Exception:\n",
        "                # skip samples that error on midi\n",
        "                print(f\"skipped idx {idx} due to midi error\")\n",
        "                pad_id = self.tokenizer.pad_token_id\n",
        "\n",
        "                return {\n",
        "                    \"input_ids\": torch.tensor([pad_id] * self.max_length, dtype=torch.long),\n",
        "                    \"attention_mask\": torch.tensor([0] * self.max_length, dtype=torch.long),\n",
        "                    \"labels\": torch.tensor([-100] * self.max_length, dtype=torch.long),\n",
        "                    \"midi_mask\": torch.tensor([0] * self.max_length, dtype=torch.long)\n",
        "                }\n",
        "\n",
        "        # merge caption and midi tokens for input\n",
        "        midi_section = \" \".join(midi_tokens)\n",
        "        full_text = caption + \" \" + midi_section\n",
        "        tokenized = self.tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            add_special_tokens=False\n",
        "        )\n",
        "        input_ids = tokenized[\"input_ids\"]\n",
        "        attention_mask = tokenized[\"attention_mask\"]\n",
        "\n",
        "        # locate midi start marker in input_ids\n",
        "        midi_start_id = self.tokenizer.convert_tokens_to_ids(\"<MIDI_START>\")\n",
        "        try:\n",
        "            midi_start_idx = input_ids.index(midi_start_id)\n",
        "        except ValueError:\n",
        "            midi_start_idx = len(input_ids)\n",
        "\n",
        "        # only predict midi tokens, ignore caption in loss\n",
        "        labels = [-100] * midi_start_idx + input_ids[midi_start_idx:]\n",
        "        labels = labels[:self.max_length]\n",
        "\n",
        "        # pad sequences to fixed length\n",
        "        pad_len = self.max_length - len(input_ids)\n",
        "        input_ids += [self.tokenizer.pad_token_id] * pad_len\n",
        "        attention_mask += [0] * pad_len\n",
        "        labels += [-100] * pad_len\n",
        "\n",
        "        # build midi_mask to flag midi positions\n",
        "        midi_mask = [0] * self.max_length\n",
        "        for i in range(midi_start_idx, self.max_length):\n",
        "            midi_mask[i] = 1\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
        "            \"midi_mask\": torch.tensor(midi_mask, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "5qucux6sSnys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Part 2: Training\n",
        "---"
      ],
      "metadata": {
        "id": "fmQ_KTkz1r8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expand Llama 3 model vocabulary to incorporate MIDI:"
      ],
      "metadata": {
        "id": "Gg94tv3f10kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_tokenizer(remi_vocab_file: str):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "    # use eos token as pad token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # midi start and end tokens\n",
        "    special_tokens = [\"<MIDI_START>\", \"<MIDI_END>\"]\n",
        "\n",
        "    # add remi vocab collected from data\n",
        "    if remi_vocab_file is not None and os.path.exists(remi_vocab_file):\n",
        "        with open(remi_vocab_file, \"r\") as f:\n",
        "            remi_tokens = [line.strip() for line in f if line.strip()]\n",
        "        special_tokens.extend(remi_tokens)\n",
        "\n",
        "    # expand tokenizer\n",
        "    tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
        "\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "X289DPpTS7Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Llama 3 model:"
      ],
      "metadata": {
        "id": "EtJNgcNo2AEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_model(tokenizer):\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"meta-llama/Llama-3.2-1B\",\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16, # for speed\n",
        "        use_cache=False # for space\n",
        "    )\n",
        "    # update tokenizer to our expanded one\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # dropout to encourage even token distribution\n",
        "    model.config.hidden_dropout_prob = 0.15\n",
        "    model.config.attention_probs_dropout_prob = 0.15\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "l8RVz_B-1_hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MIDI Trainer\n",
        "This custom trainer is primarily used to increase loss on the midi part of the training."
      ],
      "metadata": {
        "id": "HPtiZihqOiOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMidiTrainer(Trainer):\n",
        "    def __init__(self, midi_loss_weight=3.0, inv_weights=None, tokenizer=None, **kwargs):\n",
        "        # loss multiplier for midi tokens\n",
        "        self.midi_loss_weight = midi_loss_weight\n",
        "        self.inv_weights = inv_weights\n",
        "        self._tokenizer = tokenizer\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        # forward pass to get logits\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        labels = inputs[\"labels\"]  # padded label ids with -100 for ignore\n",
        "\n",
        "        # compute token cross entropy loss without reduction\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(\n",
        "            reduction=\"none\", ignore_index=-100, label_smoothing=0.05\n",
        "        )\n",
        "        # flatten for loss then reshape back\n",
        "        loss = loss_fct(\n",
        "            logits.view(-1, logits.size(-1)), labels.view(-1)\n",
        "        ).view(labels.size())\n",
        "\n",
        "        midi_mask = inputs.get(\"midi_mask\")\n",
        "\n",
        "        # apply custom weighting if midi mask and inv_weights available\n",
        "        if midi_mask is not None and self.inv_weights and self._tokenizer:\n",
        "            vocab_size = len(self._tokenizer)\n",
        "            device = labels.device\n",
        "\n",
        "            # start with ones and set inverse frequency weights\n",
        "            weights_vector = torch.ones(vocab_size, device=device)\n",
        "            for token_id, weight in self.inv_weights.items():\n",
        "                if token_id < vocab_size:\n",
        "                    weights_vector[token_id] = weight\n",
        "\n",
        "            # select weights per label\n",
        "            token_weights = weights_vector[labels]\n",
        "\n",
        "            # boost midi token loss by midi_loss_weight\n",
        "            final_weights = torch.where(\n",
        "                midi_mask == 1,\n",
        "                self.midi_loss_weight * token_weights,\n",
        "                torch.ones_like(token_weights)\n",
        "            )\n",
        "            loss = loss * final_weights\n",
        "\n",
        "        # average over all tokens\n",
        "        loss = loss.mean()\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "y7As-oavTORw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_inverse_frequency_weights(samples, tokenizer, smooth: int = 1):\n",
        "    # count occurrences of token ids\n",
        "    counter = Counter()\n",
        "\n",
        "    for s in samples:\n",
        "        # tokenize caption without special tokens\n",
        "        cap_ids = tokenizer(s[\"caption\"], add_special_tokens=False).input_ids\n",
        "        counter.update(cap_ids)\n",
        "\n",
        "        midi = s.get(\"midi_tokens\", [])\n",
        "        if isinstance(midi, str):\n",
        "            midi_list = midi.split()\n",
        "        else:\n",
        "            midi_list = midi\n",
        "\n",
        "        # convert midi tokens to ids\n",
        "        midi_ids = tokenizer.convert_tokens_to_ids(midi_list)\n",
        "        counter.update(midi_ids)\n",
        "\n",
        "    total = sum(counter.values())\n",
        "\n",
        "    # compute inverse weights\n",
        "    inv = {tok: total / (count + smooth) for tok, count in counter.items()}\n",
        "\n",
        "    # normalize\n",
        "    mean_w = sum(inv.values()) / len(inv)\n",
        "    inv = {tok: w / mean_w for tok, w in inv.items()}\n",
        "    return inv"
      ],
      "metadata": {
        "id": "0Ikd5kF25aDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetune Llama 3 on the MIDICaps Data:"
      ],
      "metadata": {
        "id": "p05gHMpF16Xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tensorboard:"
      ],
      "metadata": {
        "id": "e2cBurm-Tg71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs"
      ],
      "metadata": {
        "id": "cNYNjnBVTgn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Loop:"
      ],
      "metadata": {
        "id": "Im_uFZj9TngM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    base_path = \"/content/\"\n",
        "    json_path = os.path.join(base_path, \"train.json\")\n",
        "    cleaned_vocab_file = os.path.join(base_path, \"remi_vocab_cleaned_100.txt\")\n",
        "    valid_samples_file = os.path.join(base_path, \"valid_samples_100.json\")\n",
        "    checkpoint_dir = \"./llama_midicaps_model_final7\"\n",
        "\n",
        "    # load dataset from json file\n",
        "    dataset_dict = load_dataset(\"json\", data_files=json_path)[\"train\"]\n",
        "\n",
        "    midi_tokenizer = REMI()\n",
        "\n",
        "    # resume from save if available\n",
        "    if os.path.exists(valid_samples_file) and os.path.exists(cleaned_vocab_file):\n",
        "        valid_samples = load_valid_samples(valid_samples_file)\n",
        "        with open(cleaned_vocab_file, \"r\") as f:\n",
        "            remi_vocab = [line.strip() for line in f if line.strip()]\n",
        "    else:\n",
        "        # clean dataset and build remi vocab\n",
        "        valid_samples, remi_vocab = clean_dataset_and_build_vocab(\n",
        "            hf_dataset=dataset_dict,\n",
        "            base_path=base_path,\n",
        "            midi_tokenizer=midi_tokenizer,\n",
        "            midi_max_length=256,\n",
        "            sample_limit=100000\n",
        "        )\n",
        "        save_vocab(remi_vocab, cleaned_vocab_file)\n",
        "        save_valid_samples(valid_samples, valid_samples_file)\n",
        "\n",
        "    # split data\n",
        "    cleaned_dataset = HFDataset.from_list(valid_samples)\n",
        "    split_dataset = cleaned_dataset.train_test_split(test_size=0.1, seed=6640)\n",
        "    train_dataset_hf = split_dataset[\"train\"]\n",
        "\n",
        "    tokenizer = prepare_tokenizer(cleaned_vocab_file)\n",
        "    model = prepare_model(tokenizer)\n",
        "\n",
        "    # compute token weights to emphasize rare tokens\n",
        "    inv_weights = compute_inverse_frequency_weights(valid_samples, tokenizer)\n",
        "\n",
        "    # wrap data into custom dataset class\n",
        "    train_dataset = MidicapsMIDIDataset(\n",
        "        train_dataset_hf,\n",
        "        base_path,\n",
        "        tokenizer,\n",
        "        midi_tokenizer,\n",
        "        max_length=512,\n",
        "        midi_max_length=256\n",
        "    )\n",
        "\n",
        "    # setup training args\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=checkpoint_dir,\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        warmup_ratio=0.2,\n",
        "        logging_steps=50,\n",
        "        save_steps=200,\n",
        "        save_total_limit=2,\n",
        "        bf16=True,\n",
        "        report_to=[\"tensorboard\"],\n",
        "        logging_dir=\"./logs\",\n",
        "        max_grad_norm=1.0\n",
        "    )\n",
        "\n",
        "    # initialize custom trainer with midi loss\n",
        "    trainer = CustomMidiTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=default_data_collator,\n",
        "        midi_loss_weight=2.0,\n",
        "        inv_weights=inv_weights\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # local save\n",
        "    save_path = \"./final_llama_midicaps_model\"\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "\n",
        "    # drive backup save\n",
        "    second_save_path = \"/content/drive/MyDrive/Final-Model-LexLull\"\n",
        "    os.makedirs(second_save_path, exist_ok=True)\n",
        "    model.save_pretrained(second_save_path)\n",
        "    tokenizer.save_pretrained(second_save_path)\n",
        "\n",
        "\n",
        "train()\n"
      ],
      "metadata": {
        "id": "8EmOUskTTadA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Evaluation"
      ],
      "metadata": {
        "id": "NX14QEz8_uNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"/content/checkpoint-6500\"\n",
        "valid_json = \"/content/valid_samples_100.json\"\n",
        "batch_size = 64\n",
        "\n",
        "# load test data\n",
        "with open(valid_json) as f:\n",
        "    samples = json.load(f)\n",
        "random.seed(6640) # in honor of cap6640\n",
        "\n",
        "# few shot for context\n",
        "few_shot = [\n",
        "    \"A gentle lullaby in C major. <MIDI_START> Program_0 Pitch_60 Duration_10 Velocity_40 <MIDI_END>\",\n",
        "    \"Upbeat rock riff with power chords. <MIDI_START> Program_30 Pitch_50 Duration_5 Velocity_70 <MIDI_END>\",\n",
        "    \"Funk bass line with syncopation. <MIDI_START> Program_40 Pitch_35 Duration_3 Velocity_80 <MIDI_END>\"\n",
        "]\n",
        "context = \" \".join(few_shot)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir, padding_side=\"left\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_dir).to(device)\n",
        "model = model.half()\n",
        "model.eval()\n",
        "\n",
        "# generate predictions\n",
        "prompts, generated_sequences, reference_sequences = [], [], []\n",
        "for i in tqdm(range(0, len(samples), batch_size), desc=\"generating\"):\n",
        "    batch = samples[i : i + batch_size]\n",
        "    batch_prompts = [f\"{context} {s['caption'].strip()} <MIDI_START>\" for s in batch]\n",
        "    enc = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    out = model.generate(\n",
        "        enc.input_ids,\n",
        "        attention_mask=enc.attention_mask,\n",
        "        max_new_tokens=256,\n",
        "        repetition_penalty=1.2,\n",
        "        no_repeat_ngram_size=3,\n",
        "        eos_token_id=tokenizer.convert_tokens_to_ids(\"<MIDI_END>\"),\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "    for j, seq in enumerate(out):\n",
        "        gen_ids = seq.tolist()[enc.input_ids.shape[1]:]\n",
        "        tokens = tokenizer.convert_ids_to_tokens(gen_ids)\n",
        "        generated_sequences.append(\" \".join(tokens))\n",
        "        prompts.append(batch_prompts[j])\n",
        "        reference_sequences.append(\" \".join(batch[j].get(\"midi_tokens\", [])))\n",
        "\n",
        "# compute bleu and rouge scores\n",
        "bleu_score = load(\"bleu\").compute(predictions=generated_sequences, references=[[r] for r in reference_sequences])[\"bleu\"] * 100\n",
        "rouge_score = load(\"rouge\").compute(predictions=generated_sequences, references=reference_sequences)[\"rougeL\"] * 100\n",
        "\n",
        "print(f\"bleu-4: {bleu_score:.2f}\")\n",
        "print(f\"rouge-l: {rouge_score:.2f}\")\n",
        "\n",
        "for k in range(5):\n",
        "    print(f\"prompt: {prompts[k]}\")\n",
        "    print(f\"gen: {generated_sequences[k]}\")\n"
      ],
      "metadata": {
        "id": "I8Ayi15KTylm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}